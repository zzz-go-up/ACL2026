import re
import functools
import time
import asyncio
import httpx
import numpy as np
import ray
from langdetect import detect, DetectorFactory
from langdetect.lang_detect_exception import LangDetectException
import string
import regex
from collections import Counter
import fasttext  

FASTTEXT_MODEL_PATH = "/path/to/your/model/lid.176.bin" 
_LANG_MODEL = None  


DetectorFactory.seed = 42


JUDGE_PROMPT = """
### System Role
You are an expert evaluator for Retrieval-Augmented Generation (RAG) systems, specializing in biomedical (BioASQ) and multi-hop reasoning (HotpotQA) domains. 
Your task is to assess the quality of a **Student Model's Answer** given a **Question**, the **Retrieved Context**, and a **Reference Answer**.

### Input Data
1. **Question and Context**: The user's query and the retrieved context.
2. **Reference Answer**: The ground truth answer (Gold Standard).
3. **Student Answer**: The response generated by the model being trained.

### Evaluation Criteria
You must evaluate the Student Answer based on the following dimensions:

1.  **Faithfulness (Hallucination Check)**: 
    - Does the answer rely *only* on the provided Context? 
    - Does it fabricate information not present in the Context? (Penalty: High)

2.  **Correctness (Alignment)**: 
    - Does the answer match the intent and key facts of the Reference Answer?
    - For BioASQ: Are the specific biomedical entities (drugs, genes, diseases) correct?
    - For HotpotQA: Is the reasoning chain complete?

3.  **Conciseness**: 
    - Is the answer direct? Does it avoid "fluff" (e.g., "Based on the context provided...")?

### Scoring Rubric (Range: 0.0 to 1.0)
Assign a score based on strict adherence to the rubric below:

- **1.0 (Perfect)**: The answer is factually correct, fully supported by context, matches the reference, and is concise.
- **0.8 (Good)**: Factually correct and supported, but contains minor verbosity or slightly misses the nuance of the reference.
- **0.5 (Partial)**: Partially correct (e.g., answers one part of a multi-hop question) OR contains minor extrinsic information not in context.
- **0.2 (Poor)**: Misses the key entity/answer but attempts to address the topic.
- **0.0 (Wrong/Hallucinated)**: Factually incorrect, contradicts the context, or irrelevant.

### Output Format
You must output your response in the following strict XML format. Do not use Markdown code blocks.

<reasoning>
[Step 1] Compare Student Answer vs. Reference Answer.
[Step 2] Check if the evidence exists in the Context.
[Step 3] Assess conciseness and style.
[Final Conclusion] Justify the final score.
</reasoning>
<score>
[A single float value between 0.0 and 1.0]
</score>

---
### Task
**Question and Context**: {question}


**Reference Answer**: {reference}

**Student Answer**: {model_response}

Begin your evaluation now.
"""


def calculate_3gram_recall_score(prediction, reference):
   
    def normalize(text: str) -> str:
        if not text:
            return ""
        text = text.lower()
        text = regex.sub(r"\b(a|an|the)\b", " ", text)
        text = regex.sub(r"[^\w\s]", " ", text)
        return " ".join(text.split())


    def get_ngrams(s: str, n: int = 3):
        tokens = []
        for w in s.split():
            l = len(w)
            if l < n:
                tokens.append(w)
            else:
                for i in range(l - n + 1):
                    tokens.append(w[i : i + n])
        return tokens


    pred_norm = normalize(prediction)
    ref_norm = normalize(reference)
    
    pred_tokens = get_ngrams(pred_norm, n=3)
    ref_tokens = get_ngrams(ref_norm, n=3)
    
    if not ref_tokens:
        return 0.0
    
    pred_counter = Counter(pred_tokens)
    ref_counter = Counter(ref_tokens)
    
    common = pred_counter & ref_counter
    num_same = sum(common.values())
    
    recall = 1.0 * num_same / len(ref_tokens)
    
    return recall

def detect_language_fasttext(text, lang_model):
    if not text or not lang_model:
        return "unknown"
    
    clean_text = text.replace("\n", " ").strip()
    if not clean_text:
        return "unknown"

    try:
        prediction = lang_model.predict(clean_text, k=1)
        label = prediction[0][0]
        lang_code = label.replace("__label__", "")
        return lang_code
    except Exception:
        return "unknown"

def check_language_consistency(pred_text, ref_text, lang_model):
    if len(pred_text) < 10 or len(ref_text) < 10:
        return True
        
    if not lang_model:
        return True

    try:
        ref_lang = detect_language_fasttext(ref_text, lang_model)
        pred_lang = detect_language_fasttext(pred_text, lang_model)

        if ref_lang == 'id' and pred_lang != 'id':
            return False
            
        if ref_lang == 'zh' and pred_lang != 'zh':
            return False
            
        if ref_lang != pred_lang:
            return False
            
    except Exception as e:
        print(f"Lang check error: {e}")
        return True 
        
    return True

def extract_reward(judge_output):
    try:
        match = re.search(r"<score>\s*([\d\.]+)\s*</score>", judge_output)
        if match:
            score = float(match.group(1))
            return max(0.0, min(1.0, score))
        
        print("Judge error: fallback to 3 gram score")
        return 2.0
    
    except Exception as e:
        print(f"Error parsing reward: {e}")
        return 2.0



async def compute_score_single_async(client, data_source, solution_str, ground_truth, lang_model, extra_info=None):
    if "</think>" in solution_str:
        answer_pred = solution_str.split("</think>")[-1].strip()
    else:
        answer_pred = solution_str.strip()

    if not answer_pred:
        return {"score": 0.0, "acc_score": 0.0, "len_score": 0.0, "cta_score": 0.0, "lang_score": 0.0}

    ref_answer = ground_truth[0] if isinstance(ground_truth, list) else ground_truth

    acc_score = calculate_3gram_recall_score(answer_pred, ground_truth)
    
    lang_score = 0.0
    is_lang_match = check_language_consistency(answer_pred, ref_answer, lang_model)
    if is_lang_match:
        lang_score = 1  

    len_score = 1.0
    total_len = len(answer_pred)
    ref_len = len(ref_answer)
    if total_len > 3500 :
        penalty = 0.0001 * (total_len - 3500)
        len_score = max(0.0, 1.0 - penalty)

    cta_score = 0.0
    if extra_info and total_len > 0:
        try:
            user_question = extra_info.get("en_question", "User Question")
            prompt = JUDGE_PROMPT.format(
                question=user_question,
                reference=ref_answer,
                model_response=answer_pred
            )

            response = await client.post("/chat/completions", json={
                "model": "qwen3-judge",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.0,
                "max_tokens": 2048
            }, timeout=120.0)
            
            content = response.json()['choices'][0]['message']['content']
            cta_score = extract_reward(content)

            if content is None or cta_score == 2.0:
                print("Judge Error: Empty content or zero score from judge.")
                cta_score = acc_score

        except Exception as e:
            print(f"Judge Error: {e}")
            cta_score = acc_score  

    
    final_score = cta_score + acc_score + len_score + 0.5*lang_score
    return {
        "score": final_score,
        "acc_score": acc_score,
        "len_score": len_score,
        "cta_score": cta_score,
        "lang_score": lang_score,
        "total_len": total_len
    }



@ray.remote
def run_ray_batch(chunk_data):
    data_sources, solution_strs, ground_truths, extra_infos = chunk_data

    global _LANG_MODEL
    if _LANG_MODEL is None:
        import fasttext
        _LANG_MODEL = fasttext.load_model(FASTTEXT_MODEL_PATH)
    
    async def process_all():
        async with httpx.AsyncClient(base_url="http://your-api-url/v1", timeout=120.0) as client:
            tasks = [compute_score_single_async(client, ds, ss, gt, _LANG_MODEL, ei) 
                     for ds, ss, gt, ei in zip(data_sources, solution_strs, ground_truths, extra_infos)]
            return await asyncio.gather(*tasks)
            
    return asyncio.run(process_all())

def compute_score_R_acc_lc_cta_batch_async_chunk(data_sources, solution_strs, ground_truths, extra_infos=None) -> list[dict]:
    if extra_infos is None: extra_infos = [{} for _ in data_sources]
    
    num_processes = 4
    split_indices = np.array_split(range(len(data_sources)), num_processes)
    chunks = [( [data_sources[i] for i in idx], [solution_strs[i] for i in idx],
                [ground_truths[i] for i in idx], [extra_infos[i] for i in idx] ) for idx in split_indices if len(idx) > 0]

    futures = [run_ray_batch.remote(c) for c in chunks]
    return [item for sublist in ray.get(futures) for item in sublist]
